{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
      "metadata": {},
      "source": [
        "# How to add persistence (\"memory\") to your graph\n",
        "\n",
        "Many AI applications need memory to share context across multiple interactions. In LangGraph, memory is provided for any [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph) through [Checkpointers](https://github.com/langchain-ai/langgraph/tree/e4ca7ab69c599fd77dd4f0d47280849d715392cc/libs/checkpoint).\n",
        "\n",
        "When creating any LangGraph workflow, you can set them up to persist their state by doing using the following:\n",
        "\n",
        "1. A [Checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#basecheckpointsaver).\n",
        "2. Call `compile(checkpointer=my_checkpointer)` when compiling the graph.\n",
        "\n",
        "There are several options for checkpointers to use.\n",
        "\n",
        "1. [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#memorysaver) is an in-memory key-value store for Graph state.\n",
        "2. [SqliteSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#sqlitesaver) allows you to save to a Sqlite db locally or in memory.\n",
        "3. There are various external databases that can be used for persistence, such as [Postgres](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/), [MongoDB](https://langchain-ai.github.io/langgraph/how-tos/persistence_mongodb/), and [Redis](https://langchain-ai.github.io/langgraph/how-tos/persistence_redis/).\n",
        " \n",
        "Here is an example using [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#memorysaver) in memory:\n",
        "```python\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "builder = StateGraph(....)\n",
        "# ... define the graph\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=memory)\n",
        "...\n",
        "```\n",
        "\n",
        "This works for [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph) and all its subclasses, such as [MessageGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#messagegraph).\n",
        "\n",
        "Below is an example.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the <code>create_react_agent(model, tools=tool, checkpointer=checkpointer)</code> (<a href=\"https://langchain-ai.github.io/langgraph/reference/prebuilt/#create_react_agent\">API doc</a>) constructor. This may be more appropriate if you are used to LangChain’s <a href=\"https://python.langchain.com/docs/how_to/agent_executor/#concepts\">AgentExecutor</a> class.\n",
        "    </p>\n",
        "</div>    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cbd446a-808f-4394-be92-d45ab818953c",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First we need to install the packages required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0abe11f4-62ed-4dc4-8875-3db21e260d1d",
      "metadata": {},
      "source": [
        "Next, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the search tool we will use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
      "metadata": {},
      "source": [
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
        "    <p style=\"padding-top: 5px;\">\n",
        "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
        "    </p>\n",
        "</div>    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf509bc",
      "metadata": {},
      "source": [
        "## Define graph state\n",
        "\n",
        "The state is the interface for all the nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "14619607",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Add messages essentially does this with more\n",
        "# robust handling\n",
        "# def add_messages(left: list, right: list):\n",
        "#     return left + right\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ac643b-cb06-4724-a80c-2862ba4773f1",
      "metadata": {},
      "source": [
        "## Define tools\n",
        "\n",
        "We will first define the tools we want to use.\n",
        "For this simple example, we will use create a placeholder search engine.\n",
        "However, it is really easy to create your own tools - see documentation [here](https://python.langchain.com/docs/how_to/custom_tools) on how to do that.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d7ef57dd-5d6e-4ad3-9377-a92201c1310e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def search(query: str):\n",
        "    \"\"\"Call to surf the web.\"\"\"\n",
        "    # This is a placeholder for the actual implementation\n",
        "    return [\"The answer to your question lies within.\"]\n",
        "\n",
        "\n",
        "tools = [search]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01885785-b71a-44d1-b1d6-7b5b14d53b58",
      "metadata": {},
      "source": [
        "Now we can create our [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tool+node#toolnode). This \n",
        "object actually **runs** the tools (aka functions)  that the LLM has asked to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5cf3331e-ccb3-41c8-aeb9-a840a94d41e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "tool_node = ToolNode(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5497ed70-fce3-47f1-9cad-46f912bad6a5",
      "metadata": {},
      "source": [
        "## Define the model\n",
        "\n",
        "Now we need to load the [chat model](https://python.langchain.com/docs/concepts/#chat-models) to power our agent.\n",
        "For the design below, it must satisfy two criteria:\n",
        "\n",
        "1. It should work with **messages** (since our state contains a list of chat messages)\n",
        "2. It should work with [**tool calling**](https://python.langchain.com/docs/concepts/#functiontool-calling).\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
        "    </p>\n",
        "</div>    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "892b54b9-75f0-4804-9ed0-88b5e5532989",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We will set streaming=True so that we can stream tokens\n",
        "# See the streaming section for more information on this.\n",
        "model = ChatOpenAI(temperature=0, streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77995c0-bae2-4cee-a036-8688a90f05b9",
      "metadata": {},
      "source": [
        "\n",
        "After we've done this, we should make sure the model knows that it has these tools available to call.\n",
        "We can do this by converting the LangChain tools into the format for OpenAI function calling, and then bind them to the model class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cd3cbae5-d92c-4559-a4aa-44721b80d107",
      "metadata": {},
      "outputs": [],
      "source": [
        "bound_model = model.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e03c5094-9297-4d19-a04e-3eedc75cefb4",
      "metadata": {},
      "source": [
        "## Define nodes and edges \n",
        "\n",
        "We now need to define a few different nodes in our graph.\n",
        "In `langgraph`, a node can be either a function or a [runnable](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel).\n",
        "There are two main nodes we need for this:\n",
        "\n",
        "1. The agent: responsible for deciding what (if any) actions to take.\n",
        "2. A function to invoke tools: if the agent decides to take an action, this node will then execute that action.\n",
        "\n",
        "We will also need to define some edges.\n",
        "Some of these edges may be conditional.\n",
        "The reason they are conditional is that based on the output of a node, one of several paths may be taken.\n",
        "The path that is taken is not known until that node is run (the LLM decides).\n",
        "\n",
        "1. Conditional Edge: after the agent is called, we should either:\n",
        "   a. If the agent said to take an action, then the function to invoke tools should be called\n",
        "   b. If the agent said that it was finished, then it should finish\n",
        "2. Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next\n",
        "\n",
        "Let's define the nodes, as well as a function to decide how what conditional edge to take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3b541bb9-900c-40d0-964d-7b5dfee30667",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the function that determines whether to continue or not\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "def should_continue(state: State) -> Literal[\"action\", \"__end__\"]:\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    # If there is no function call, then we finish\n",
        "    if not last_message.tool_calls:\n",
        "        return \"__end__\"\n",
        "    # Otherwise if there is, we continue\n",
        "    return \"action\"\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: State):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffd6e892-946c-4899-8cc0-7c9291c1f73b",
      "metadata": {},
      "source": [
        "## Compile the graph\n",
        "\n",
        "We can now put it all together and define the graph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "812b4e70-4956-4415-8880-db48b3dcbad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", tool_node)\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    # First, we define the start node. We use `agent`.\n",
        "    # This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_continue,\n",
        ")\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that after `tools` is called, `agent` node is called next.\n",
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9c8536-f90b-44fa-958d-5df016c66d8f",
      "metadata": {},
      "source": [
        "### Persistence\n",
        "\n",
        "To add in persistence, we pass in a checkpoint when compiling the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6845ed6a-d155-4105-9160-28849877248b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "79d29875-8aa8-434c-9f20-1c58346a6249",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable\n",
        "app = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7654ebcc-2179-41b4-92d1-6666f6f8634f",
      "metadata": {},
      "source": [
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        If you're using LangGraph Cloud, you <strong>don't need</strong> to pass checkpointer when compiling the graph, since it's done automatically.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0d49697f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1b56c5-bd61-4192-8bdb-458a1e9f0159",
      "metadata": {},
      "source": [
        "## Use the graph\n",
        "\n",
        "We can now interact with the agent and see that it remembers previous messages!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm bob\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Bob! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
        "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "08ae8246-11d5-40e1-8567-361e5bef8917",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob.\n"
          ]
        }
      ],
      "source": [
        "input_message = HumanMessage(content=\"what is my name?\")\n",
        "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f47bbfc-d9ef-4288-ba4a-ebbc0136fa9d",
      "metadata": {},
      "source": [
        "If we want to start a new conversation, we can pass in a different thread id. Poof! All the memories are gone!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "273d56a8-f40f-4a51-a27f-7c6bb2bda0ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\n"
          ]
        }
      ],
      "source": [
        "input_message = HumanMessage(content=\"what is my name?\")\n",
        "for event in app.stream(\n",
        "    {\"messages\": [input_message]},\n",
        "    {\"configurable\": {\"thread_id\": \"3\"}},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e833f994",
      "metadata": {},
      "source": [
        "All the checkpoints are persisted to the checkpointer, so you can always resume previous threads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "8578a66d-6489-4e03-8c23-fd0530278455",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "You forgot??\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I apologize for the confusion. I am an AI assistant and I do not have the ability to remember information from previous interactions. How can I assist you today, Bob?\n"
          ]
        }
      ],
      "source": [
        "input_message = HumanMessage(content=\"You forgot??\")\n",
        "for event in app.stream(\n",
        "    {\"messages\": [input_message]},\n",
        "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
